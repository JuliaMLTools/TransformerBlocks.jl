var documenterSearchIndex = {"docs":
[{"location":"#TransformerBlocks.jl","page":"TransformerBlocks.jl","title":"TransformerBlocks.jl","text":"","category":"section"},{"location":"#Simple,-blazing-fast,-transformer-components.","page":"TransformerBlocks.jl","title":"Simple, blazing fast, transformer components.","text":"","category":"section"},{"location":"#Basic-Usage","page":"TransformerBlocks.jl","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"TransformerBlocks.jl","title":"TransformerBlocks.jl","text":"using TransformerBlocks\n\n# C: channel size (embedding dimension)\n# T: block size (sequence length)\n# B: batch size\nC, T, B = 10, 5, 3\nx = rand(Float32, C, T, B)\n\n# Example 1: Transformer block\nblock = Block(C)\n@assert size(block(x)) == (C, T, B)\n\n# Example 2: Block with mask\nusing LinearAlgebra\nmask = (1 .- triu(ones(Float32, T, T))) .* (-1f9)\n@assert size(block(x; mask=mask)) == (C, T, B)\n\n# Example 3: Sequential blocks\nnum_layers = 3\nblocks = BlockList([Block(C) for _ in 1:num_layers])\n@assert size(blocks(x)) == (C, T, B)","category":"page"},{"location":"#Components","page":"TransformerBlocks.jl","title":"Components","text":"","category":"section"},{"location":"","page":"TransformerBlocks.jl","title":"TransformerBlocks.jl","text":"TransformerBlocks.Head\nMultiheadAttention\nFeedForward\nBlock\nBlockList","category":"page"},{"location":"#TransformerBlocks.Head","page":"TransformerBlocks.jl","title":"TransformerBlocks.Head","text":"Head(input_dim, head_size; dropout=0)\n\nInitializes one instance of the Head type, representing one head of self-attention.\n\n\n\n\n\n(::Head)(x; mask=nothing)\n\nA Head instance accepts an input array x of dimensions (C, T, B) and outputs an array of dimensions (HS, T, B). \"C\" is the channel size (embedding dimension). \"T\" is the block size (number of input tokens). \"B\" is the batch size. \"HS\" is the head size.\n\nThe following keyword arguments are supported:\n\nmask (Defaults to nothing. Must be of dimensions (T, T).)\n\nExamples:\n\nC,T,B = 2,3,4\nHS = 10\nhead = Head(C,HS)\n@assert size(head(rand(C,T,B))) == (HS,T,B)\n\n\n\n\n\n","category":"type"},{"location":"#TransformerBlocks.MultiheadAttention","page":"TransformerBlocks.jl","title":"TransformerBlocks.MultiheadAttention","text":"MultiheadAttention\n\nMultiple heads of self-attention in parallel\n\n\n\n\n\n","category":"type"},{"location":"#TransformerBlocks.FeedForward","page":"TransformerBlocks.jl","title":"TransformerBlocks.FeedForward","text":"FeedForward\n\nA simple linear layer followed by a non-linearity\n\n\n\n\n\n","category":"type"},{"location":"#TransformerBlocks.Block","page":"TransformerBlocks.jl","title":"TransformerBlocks.Block","text":"Block\n\nA single transformer block: communication followed by computation\n\n\n\n\n\n","category":"type"},{"location":"#TransformerBlocks.BlockList","page":"TransformerBlocks.jl","title":"TransformerBlocks.BlockList","text":"BlockList\n\nA sequence of transformer blocks composed together\n\n\n\n\n\n","category":"type"},{"location":"#API-index","page":"TransformerBlocks.jl","title":"API index","text":"","category":"section"},{"location":"","page":"TransformerBlocks.jl","title":"TransformerBlocks.jl","text":"","category":"page"}]
}
